## About
I am a PhD student at [Machine Vision Lab](https://balarsgroup.github.io/), IIT Roorkee, where I work under [Prof. R.Balasubramanian](https://sites.google.com/site/balaiitr/) in the field of Computer Vision and Deep Learning. My research interest includes Multimodal Human Activity Recognition using RGB, depth, sensor and infrared data. 

## Publications



| <img src="https://i.ibb.co/QYYcdLj/0001.jpg" alt="Your image title" width="350"/>  | Evaluating Fusion of RGB-D and Inertial Sensors for
Multimodal Human Action Recognition
<br>
Abstract Fusion of multiple modalities from dierent sensors is an important
area of research for multimodal human action recognition. In this paper, we conduct
an in-depth study to investigate the eect of dierent parameters like input
preprocessing, data augmentation, network architectures and model fusion so as
to come up with a practical guideline for multimodal action recognition using
deep learning paradigm. First, for RGB videos, we propose a novel image-based
descriptor called Stacked Dense Flow Dierence Image (SDFDI), capable of capturing
the spatio-temporal information present in a video sequence. A variety of
deep 2D Convolutional Neural Networks (CNN) are then trained to compare our
SDFDI against state-of-the-art image-based representations. Second, for skeleton
stream, we propose data augmentation technique based on 3D transformations so
as to facilitate training a deep neural network on small datasets. We also propose
a Bidirectional Gated Recurrent Unit (BiGRU) based Recurrent Neural Network
(RNN) to model skeleton data. Third, for inertial sensor data, we propose data
augmentation based on jittering with white Gaussian noise along with deep a 1DCNN
network for action classication. The outputs of all these three heterogeneous
networks (1D-CNN, 2D-CNN and BiGRU) are combined by a variety of model fusion
approach based on score and feature fusion. Finally, in order to illustrate
the ecacy of the proposed framework, we test our model on a publicly available
UTD-MHAD dataset, and achieved an overall accuracy of 97.91%, which is about
4% higher than using each modality individually. We hope that the discussions
and conclusions from this work will provide a deeper insight to the researchers in|

| Content Cell  | Content Cell  |


